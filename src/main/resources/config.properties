# TEST PATH
# test will be created in declared folder
output.base.path = test_results/infrastructure_modified/res_type/3

# DATA SOURCE
input.file.path = /Users/simone/Documents/Tesi/profile_without_last_month.dat
input.file.path.training = /Users/simone/Documents/Tesi/profile_last_month.dat

# NODE TYPES NUMBER
node.types.number = 3

# OPERATOR
# max parallelism
operator.max.parallelism = 5
# case considered while computing response time or utilization
# types:
#   - avg
#   - worst
operator.values.computing.case = worst
# Load balancer types:
#   - rr (Round Robin)
operator.load.balancer.type = rr

# CONSTRAINTS
dsp.slo.latency = 0.065

# OPERATOR MANAGER
# types:
#   - do-nothing
#   - threshold
#   - q-learning
#   - fa-q-learning (Q-Learning with function approximation)
#   - deep-q-learning
#   - deep-v-learning
#   - vi (value iteration)
#   - fa-tb-vi (trajectory based value iteration using linear function approximation)
#   - deep-tb-vi (trajectory based value iteration using deep reinforcement learning)
#   - fa-hybrid (fa-tb-vi om for off-line training and fa-q-learning for on-line training)
#   - deep-hybrid (deep-tb-vi om for off-line training and deep-v-learning for on-line training)
edf.om.type = deep-v-learning

# THRESHOLD BASED OM PARAMS
edf.om.threshold = 0.8

# REWARD BASED OM PARAMS
edf.rl.om.reconfiguration.weight = 0.4
edf.rl.om.slo.weight = 0.4
edf.rl.om.resources.weight = 0.2
# max input rate considered to discretize lambda
edf.rl.om.max.input.rate = 600
# lambda levels
edf.rl.om.input.rate.levels = 30
# state representation
# types:
#   - k_lambda
#   - reduced_k_lambda
#   _ general_resources
edf.rl.om.state.representation = k_lambda

# DYNAMIC PROGRAMMING
edf.dp.gamma = 0.99

# VI OM PARAMS
edf.vi.max.iterations = 0
edf.vi.max.time.seconds = 60
edf.vi.theta = 1E-5

# TBVI OM PARAMS
edf.tbvi.exec.iterations = 300000
edf.tbvi.exec.seconds = 0
edf.tbvi.trajectory.length = 512
# TBVI with function approximation
edf.tbvi.fa.alpha = 0.1
# TBVI with deep learning
edf.tbvi.deep.memory.batch = 32

# Q-LEARNING OM PARAMS
edf.ql.om.alpha = 1.0
edf.ql.om.alpha.decay = 0.98
edf.ql.om.alpha.min.value = 0.05
edf.ql.om.alpha.decay.steps = 2500
edf.ql.om.gamma = 0.99

# DEEP-LEARNING OM PARAMS
edf.dl.om.gamma = 0.9
edf.dl.om.gamma.decay = 0.9
edf.dl.om.gamma.min.value = 0.01
edf.dl.om.gamma.decay.steps = -1
edf.dl.om.nd4j.random.seed = 1234L
edf.dl.om.enable.network.ui = false
edf.dl.samples.memory.size = 2
edf.dl.samples.memory.batch = 2

# ACTION SELECTION POLICY
# types:
#   - random
#   - greedy
#   - e-greedy (epsilon greedy)
asp.type = e-greedy

# RANDOM ASP PARAMS
asp.r.random.seed = 1234L

# E-GREEDY ASP PARAMS
asp.eg.epsilon = 1.0
asp.eg.epsilon.decay = 0.9
asp.eg.epsilon.min.value = 0.01
asp.eg.om.epsilon.decay.steps = 2500
asp.eg.epsilon.random.seed = 1234L
