# TEST PATH
# test will be created in declared folder
output.base.path = test_results/prova/
# DATA SOURCE
input.file.path = /Users/simone/Documents/Tesi/profile_without_last_month.dat
input.file.path.training = /Users/simone/Documents/Tesi/profile_last_month.dat
# NODE TYPES NUMBER
node.types.number = 3
# OPERATOR MAX PARALLELISM
operator.max.parallelism = 5
# CONSTRAINTS
dsp.slo.latency = 0.065

# OPERATOR MANAGER
# types:
#   - do-nothing
#   - threshold
#   - q-learning
#   - deep-q-learning
#   - deep-v-learning
#   - vi (value iteration)
#   - fa-tb-vi (trajectory based value iteration using linear function approximation)
#   - deep-tb-vi (trajectory based value iteration using deep reinforcement learning)
#   - hybrid (deep-tb-vi om for off-line training and deep-v-learning for on-line training)
edf.om.type = deep-tb-vi

# THRESHOLD BASED OM PARAMS
edf.om.threshold = 0.7

# RL OM PARAMS
edf.rl.om.reconfiguration.weight = 0.33
edf.rl.om.slo.weight = 0.33
edf.rl.om.resources.weight = 0.33
# max input rate considered to discretize lambda
edf.rl.om.max.input.rate = 600
# lambda levels
edf.rl.om.input.rate.levels = 30
# state representation
# types:
#   - k_lambda
#   - reduced_k_lambda
#   _ general_resources
edf.rl.om.state.representation = k_lambda

# Q-LEARNING OM PARAMS
edf.ql.om.alpha = 0.01
edf.ql.om.alpha.decay = 0.9
edf.ql.om.alpha.decay.steps = -1

# DEEP-LEARNING OM PARAMS
edf.dl.om.gamma = 0.99
edf.dl.om.gamma.decay = 0.9
edf.dl.om.gamma.decay.steps = -1
edf.dl.om.nd4j.random.seed = 1234L
edf.dl.om.enable.network.ui = false

# ACTION SELECTION POLICY
# types:
#   - random
#   - greedy
#   - e-greedy (epsilon greedy)
asp.type = e-greedy

# RANDOM ASP PARAMS
asp.r.random.seed = 1234L

# E-GREEDY ASP PARAMS
asp.eg.epsilon = 0.05
asp.eg.epsilon.decay = 0.9
asp.eg.om.epsilon.decay.steps = -1
asp.eg.epsilon.random.seed = 1234L
